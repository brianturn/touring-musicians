{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Twitter API with Tweepy, a Python wrapper, to search for all mentions of bands on XL Recordings (searchQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import requests\n",
    "from requests_oauthlib import OAuth1\n",
    "import tweepy, cnfg, sys, os, json\n",
    "import pandas as pd\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = cnfg.load(\"./.twitter_config\")\n",
    "consumer_key = config['consumer_key']\n",
    "consumer_secret = config['consumer_secret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    " \n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True\n",
    "                )\n",
    " \n",
    "if (not api):\n",
    "    print (\"Can't Authenticate\")\n",
    "    sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searchQuery = [('@adele', 'Adele'), ('#adele', 'Adele'), ('@radiohead', 'Radiohead'), ('#radiohead', 'Radiohead'), ('@TheAvalanches', 'The Avalanches'),('@IbeyiOfficial', 'Ibeyi'), ('@XLRECORDINGS', 'XL Recordings'), ('#xlrecordings', 'XL Recordings'),\n",
    "              ('@gloriousunseen', 'Jack White'), ('@GILA_____', 'GILA'), ('@jai_paul', 'Jai Paul'), ('#jaipaul', 'Jai Paul'), ('Jai AND Paul', 'Jai Paul'), ('@KAYTRANADA', 'Kay Tranada'), ('#kaytranada', 'Kay Tranada'), ('#kingkrule', 'Kingkrule'),\n",
    "              ('King AND Krule', 'Kingkrule'), ('@MusicLapsley', 'Lapsley'), ('#Lapsley', 'Lapsley'), ('Radiohead', 'Radiohead'), ('@ratatatmusic', 'Ratatat'), ('Ratatat', 'Ratatat'),\n",
    "              ('@matsoR', 'ROSTAM'), ('ROSTAM', 'ROSTAM'), ('@sigurros', 'Sigur Ros'), ('Sigur Ros', 'Sigur Ros'), ('#sigurros', 'Sigur Ros'), ('@PaulWoolford', 'Paul Woolford'), ('@thomyorke', 'Thom Yorke'),('#thomyorke', 'Thom Yorke'),\n",
    "              ('@vampireweekend', 'Vampire Weekend'), ('Thom Yorke', 'Thom Yorke'), ('#vampireweekend', 'Vampire Weekend'), ('@arzE', 'Ezra Koenig'), ('Ezra AND Koenig', 'Ezra Koenig'), ('@ZombyMusic', 'Zomby Music'), ('@qotsa', 'Queens of the Stone Age'),('#qotsa', 'Queens of the Stone Age'), ('Queens of the Stone Age', 'Queens of the Stone Age')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I modified code from a blog post here so that I could loop through multiple search queries and write to csv file\n",
    "#### Blog: https://www.karambelkar.info/2015/01/how-to-use-twitters-search-rest-api-most-effectively./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "maxTweets = 200000\n",
    "tweetsPerQry = 100  # this is the max the API permits\n",
    "fName = 'xl.csv' # save tweets to csv document\n",
    "\n",
    "#go as far back as API allows\n",
    "sinceId = None\n",
    "\n",
    "# If results only below a specific ID are, set max_id to that ID.\n",
    "# else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "max_id = -1\n",
    "\n",
    "tweetCount = 0\n",
    "print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "\n",
    "with open(fName, 'w') as f:\n",
    "    writer = csv.writer(f, delimiter=',') # loop through search query and write tweets to csv\n",
    "    for i in searchQuery:\n",
    "        while tweetCount < maxTweets:\n",
    "\n",
    "            try:\n",
    "                if (max_id <= 0):\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = api.search(q=i[0], count=tweetsPerQry)\n",
    "                    else:\n",
    "                        new_tweets = api.search(q=i[0], count=tweetsPerQry,\n",
    "                                                since_id=sinceId)\n",
    "                else:\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = api.search(q=i[0], count=tweetsPerQry,\n",
    "                                                max_id=str(max_id - 1))\n",
    "                    else:\n",
    "                        new_tweets = api.search(q=i[0], count=tweetsPerQry,\n",
    "                                                max_id=str(max_id - 1),\n",
    "                                                since_id=sinceId)\n",
    "                    if not new_tweets:\n",
    "                        max_id = -1\n",
    "                        #next(iterator)\n",
    "                        print('No more tweets for this search term')\n",
    "                        break\n",
    "                        \n",
    "                for tweet in new_tweets:\n",
    "                    tweet = tweet._json\n",
    "                    #print (json.dumps(tweet, indent=4, sort_keys=True))\n",
    "                    tweet = tweet._json\n",
    "                    tweet_id = tweet['id']\n",
    "                    tweet_date = tweet['created_at']\n",
    "                    tweet_location = tweet['user']['location']\n",
    "                    tweet_text = tweet['text']\n",
    "                    tweet_username = tweet['user']['screen_name']\n",
    "                    tweet_retweets = tweet['retweet_count']\n",
    "                    tweet_favorites = tweet['favorite_count']\n",
    "                    tweet_followers = tweet['user']['followers_count']\n",
    "                    tweet_friends = tweet['user']['friends_count']\n",
    "                    tweet_hashtags = tweet['entities']['hashtags']\n",
    "                    tweet_description = tweet['user']['description']\n",
    "                    tweet_reply_to = tweet['in_reply_to_status_id']\n",
    "                    search_term = i[0]\n",
    "                    artist = i[1]\n",
    "                    \n",
    "                    writer.writerow([tweet_id, tweet_date, tweet_location, tweet_text, tweet_username,\n",
    "                                     tweet_retweets, tweet_favorites, tweet_followers, tweet_friends,\n",
    "                                     tweet_hashtags, tweet_description, tweet_reply_to, search_term, artist])\n",
    "                tweetCount += len(new_tweets)\n",
    "                print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "                max_id = new_tweets[-1].id\n",
    "            except tweepy.TweepError as e:\n",
    "                print(\"some error : \" + str(e))\n",
    "                break\n",
    "      \n",
    "\n",
    "print (\"Downloaded {0} tweets, Saved to {1}\".format(tweetCount, fName))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('xl.csv', header=None, names=['id', 'created_at', 'location', 'text', 'screen_name', 'retweet_count', 'favorite_count', 'followers_count', 'friends_count', 'hashtags_text', 'description', 'tweet_reply_to', 'search_term', 'artist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(['id'], keep='last').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46879"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.location.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('xl.pkl', 'wb') as picklefile:\n",
    "#     pickle.dump(df, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
